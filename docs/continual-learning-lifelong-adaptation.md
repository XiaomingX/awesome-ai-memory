# Continual Learning & Lifelong Adaptation

确保大模型在不断学习新领域知识或适应新用户任务时，能够维持历史记忆，避免“灾难性遗忘 (Catastrophic Forgetting)”。

## 核心算法与框架

| 项目名称 | 核心算法 | 关键应用场景 |
| :--- | :--- | :--- |
| **ContinualLM** | 领域自适应预训练 | 支持领域语料增量学习，在保持既有任务性能的同时快速吸收新知识。 |
| **CURLoRA** | CUR 分解 LoRA | 低秩分解的持续微调方案，稳定参数更新，实现极低遗忘率。 |
| **AdaptiveConsistency** | 自一致性动态采样 | 动态调整推理采样策略，使模型在适应新语料时保持输出逻辑一致。 |
| **SSR** | 自合成排练 (ACL 2024) | 利用 LLM 生成合成实例进行“回放”，有效抵消新旧知识冲突。 |
| **Awesome Lifelong LLM** | 终身学习资源集 | 收录了 EWC (弹性权重固着)、回放缓冲等经典与前沿防遗忘算法。 |

## 集成建议
1. **策略选择**：对于动态运营数据，优先考虑 **CURLoRA** 或 **ContinualLM** 的增量微调模式。
2. **遗忘监控**：在适应过程中实时监控历史任务的准确率，通过**回放缓冲 (Replay Buffer)** 机制维持核心知识稳定性。
3. **参数隔离**：结合 EWC 正则化项，对重要参数进行“固着”，使其在新场景学习中不发生剧烈偏移。
