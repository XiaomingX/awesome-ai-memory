# Online Training & Robust Alignment

在大模型进行在线微调（如 RLHF/RLAIF）时，防止由于对抗性输入或偏好过度优化导致的质量雪崩和价值观一致性漂移。

## 核心推荐项目

| 项目名称 | 核心方法 | 关键特性 |
| :--- | :--- | :--- |
| **OpenRLHF** | 分布式 RLHF (PPO/GRPO) | 支持 Ray+vLLM，内置 KL 散度约束，确保持续练习中的模型稳定性。 |
| **Online-RLHF** | 在线迭代对齐 | 实时人类反馈循环，并在 LLaMA3 等模型上验证了优于离线训练的稳定性。 |
| **Minimal-RL** | 数学推理 RL | 采用负样本过滤机制防止“熵崩溃”，特别适合逻辑推理任务的在线强化。 |
| **RA-LLM** | 鲁棒对齐检查 | 通过对抗提示测试，将 ASR (攻击成功率) 降至 10% 以下，理论保证遗忘边界。 |
| **LLM-SP** | 安全隐私对齐 | 专注于对抗指令防御，在保持实用性的同时增强模型在互动中的对齐韧性。 |

## 集成建议
1. **防止雪崩**：强烈建议在在线训练中开启 **KL 散度正则化**，并监控奖励模型的方差。
2. **反馈过滤**：结合 **RA-LLM** 等检查函数，在将用户输入存入训练队列前进行安全性与一致性验证。
3. **互动场景**：对于实时互动的 Agent（如 Moltbot），应采用异步队列控制，避免单次恶意反馈直接污染全局权重。
